---
title: "Assignment 3 - Part 2 - Diagnosing Schizophrenia from Voice"
author: "Riccardo Fusaroli"
date: "October 17, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Assignment 3 - Diagnosing schizophrenia from voice

In the previous part of the assignment you generated a bunch of "features", that is, of quantitative descriptors of voice in schizophrenia, focusing on pitch.
In the course of this assignment we will use them to try to automatically diagnose schizophrenia from voice only, that is, relying on the set of features you produced last time, we will try to produce an automated classifier.

```{r}
setwd("C:/Users/emily/OneDrive/Dokumenter/R/R - Datascripts/Assignment 3")
#Load library. #alternative load package 
library(ggplot2);library(pastecs);library(rmarkdown);library(tidyr);
library(dplyr);library(QuantPsyc);library(VIF);library(stringr);library(tidytext);library(DHARMa)
library(lme4);library(lmerTest);library(psych);library(MuMIn);library(tidyverse);library(magrittr);library(Metrics);library(caret);library(simr);library(readtext);library(crqa);library(readr);library(gtools);library(lattice);library(nlme);library(pROC)
#install.packages("pacman", dependencies = TRUE)
#Load dataset from part 1
schiData <- read.delim('dataExtractSchizo.csv', sep = ",", header = TRUE)

```


### Question 1: Can you diagnose schizophrenia from pitch range only? If so, how well?

Build a logistic regression to see whether you can diagnose schizophrenia from pitch range only.
Calculate the different performance measures (accuracy, sensitivity, specificity, PPV, NPV, ROC curve) on a logistic regression using the full dataset. Don't forget the random effects!
```{r} 

#Can you predict schizophrenia with pitch. Build basic model.
D ~ Pitch + (..)
str(df)
diagnosisModel <- glmer(diagnosis ~ 1 + scale(range) + (1|study), schiData, family = "binomial")


#install.packages("gtools")
#Lav predictions
schiData$PredictionsPerc <- predict(diagnosisModel)
#Riccardo told me these out-commented lines weren't necessary. I will chose to trust him on this!!
#Make log into percent, to resemble probabilties.
#schiData$PredictionsPerc <- inv.logit(schiData$PredictionsLogit)
#Confmatrix
#schiData$PredictionsPerc=predict(diagnosisModel)
schiData$Predictions[schiData$PredictionsPerc>0]="0"
schiData$Predictions[schiData$PredictionsPerc<=0]="1"

confu<- confusionMatrix(data = schiData$Predictions, reference = schiData$diagnosis, positive = "1")
confu


#Report area under curve, rocCurve

library(pROC)
rocCurve <- roc(response = schiData$diagnosis, predictor = schiData$PredictionsPerc)
rocCurve
#print(rocCurve$auc)
#Plot
#Rocplot <- plot(rocCurve, legacy.aex = TRUE)

#Save plot
RocplotSave <- ggsave("RocplotQ1.pdf", plot(rocCurve, legacy.aex = TRUE), device = "pdf", path = "E:/R/R scripts/Diagnosing-Schizophrenia-master/RocCurve plots", limitsize = FALSE)

```

Then cross-validate the logistic regression and re-calculate performance on the testing folds. N.B. The cross-validation functions you already have should be tweaked: you need to calculate these new performance measures.
N.B. the predict() function generates log odds (the full scale between minus and plus infinity). Log odds > 0 indicates a choice of 1, below a choice of 0.
N.B. you need to decide whether calculate performance on each single test fold or save all the prediction for test folds in one datase, so to calculate overall performance.
N.B. Now you have two levels of structure: subject and study. Should this impact your cross-validation?
```{r}

schiData$SUBJ <- as.numeric(factor(schiData$participant))

#Crossvalidate. Turn lmer into glmer, turn root/square mean (rmse) error into confmatrix and rocCurve.
k <- 10 #the number of folds. 10-fold cross validation. We break up schiData set into 10 folds, call each fold the testing set and build the model on the other 9.
schi_folds <- createFolds(unique(schiData$participant, k=k))#unique assures that no 'leakage' happens (leakage: that participant is in both training and testing)

#(accuracy, sensitivity, specificity, PPV, NPV, ROC curve)

#Train variables
trainSens = NULL
trainSpec = NULL
trainACC = NULL
trainPPV = NULL
trainNPV = NULL
trainKappa = NULL

trainAUC = NULL

#Test variables
testSens = NULL
testSpec = NULL
testACC = NULL
testPPV = NULL
testNPV = NULL
testKappa = NULL

testAUC = NULL
#n is for counting
n = 1
for(i in 1:k){
  test <- subset(schiData, (SUBJ %in% schi_folds[[i]])) #find all subjects in schiData, look in all foldere for these children in folders [in[in]]
  train<- subset(schiData, !(SUBJ %in% schi_folds[[i]])) #Do the opposite        
  #Put in model
  glmModel <- glmer(diagnosis ~ 1 + scale(range) + (1|study), train, family = "binomial")
  summary(glmModel)#Get your new model (just fit on the train data)

  #Train model on data
  #Lav predictions
  train$PredictionsLogit <- predict(glmModel, train)
  #Make log into percent, to resemble probabilties.
  #schiData$PredictionsPerc <- inv.logit(train$PredictionsLogit)
  #train$PredictionsPerc=predict(newlm1)
  train$Predictions[train$PredictionsLogit>0]="0"
  train$Predictions[train$PredictionsLogit<=0]="1"
  
  #Get your new model (just fit on the train data) 
  trainpred <- confusionMatrix(data = train$Predictions, reference = train$diagnosis, positive =    "1") 
  print(trainpred)
  #Get data, sensitivity, specificity, accuracy , Positive prediciton value, negative prediction value, kappa
  trainSens[n] = trainpred$byClass[1]
  trainSpec[n] = trainpred$byClass[2]
  trainPPV[n] = trainpred$byClass[3]
  trainNPV[n] = trainpred$byClass[4]
  trainACC[n] = trainpred$overall[1]
  trainKappa[n] = trainpred$overall[2]
  
  #Get area under curve from rocCurve 
  #print(rocCurve$auc)
  trainRocCurve <- roc(response = train$diagnosis, predictor = train$PredictionsLogit)
  trainRocCurve
  
  trainAUC[n] = trainRocCurve$auc

  TrainCross <- data.frame(trainSens, trainSpec, trainPPV, trainNPV, trainACC, trainKappa,    trainAUC)  
  
  
#_______________________________________________________________________________________
 #TEST model on test data
  
 test$PredictionsPerc <- predict(glmModel, test)
 #Make log into percent, to resemble probabilties.
 #test$PredictionsPerc <- inv.logit(test$PredictionsPerc)

 #test$PredictionsPerc=predict(glmModel)
 test$Predictions[test$PredictionsPerc>0]="0"
 test$Predictions[test$PredictionsPerc<=0]="1"

 testpred<- confusionMatrix(data = test$Predictions, reference = test$diagnosis, positive = "1")
 print(testpred)

 
 testSens[n] = testpred$byClass[1]
 testSpec[n] = testpred$byClass[2]
 testPPV[n] = testpred$byClass[3]
 testNPV[n] = testpred$byClass[4]
 testACC[n] = testpred$overall[1]
 testKappa[n] = testpred$overall[2]
 
 #Get area under curve from rocCurve 
  #print(rocCurve$auc)
 testRocCurve <- roc(response = test$diagnosis, predictor = test$PredictionsPerc)
 testRocCurve
  
 testAUC[n] = testRocCurve$auc
 
 
 TestCross <- data.frame(testSens, testSpec, testPPV, testNPV, testACC, testKappa, testAUC)

}

CrossVal <- data.frame(TrainCross, TestCross)

#Tidy up data
CrossVal2 <- dplyr::select(CrossVal, trainSens:trainAUC) %>%
  gather("Train", "Train Values")

#Tidy up data
CrossVal3 <- dplyr::select(CrossVal, testSens:testAUC) %>%
  gather("Test", "Test Values")

#Merge 
CrossValM <- bind_cols(CrossVal2, CrossVal3)

#Only report the best model from the test data results. Only report the test data. 
```
Test data for range model AUC = 0.482. Disgustingly bad.

### Question 2 - Which single acoustic predictor is the best predictor of diagnosis?
Which single predictor is the best predictor of diagnosis?

#Loop thingy 

```{r}

schiData$SUBJ <- as.numeric(factor(schiData$participant))

#Get colnames and make a list of relevant colNames
accousticFeatures = colnames(schiData)[8:15]

#number of folds
k = 20
#create folds
folds = createFolds(unique(schiData$SUBJ), k = k, list = T, returnTrain = F)
library(stringr)
n = 1

for (feature in accousticFeatures){
  print(feature)
  #Create the variables and make them empty
  trainAccuracy = NULL
  trainSensitivity = NULL
  trainSpecificity = NULL
  trainPPV = NULL
  trainNPV = NULL
  trainAUC = NULL

  testAccuracy = NULL
  testSensitivity = NULL
  testSpecificity = NULL
  testPPV = NULL
  testNPV = NULL
  testAUC = NULL
  
  #Add N for counting
  N = 1
  
    #Make the string for the string for the model
      stringModel = paste("diagnosis ~ scale(", feature, ") + (1|study)", sep = "")
  
  #Make sub-loop for CV
  
  for (fold in folds){
    testData = subset(schiData, SUBJ %in% fold)
    trainData = subset(schiData, !(SUBJ %in% fold))
    
#TRAIN model on train data_______________________________________________________________
      
    model = glmer(stringModel, trainData, family = binomial)
    
    #Predict
    trainData$Perc = predict(model, trainData)
    
    #calculate probabilities
    
   #Made tgem 0 instead of 0.5. Should make it probabilities. I still trust Riccardo!
    trainData$predictions[trainData$Perc > 0] = "1"
   
    trainData$predictions[trainData$Perc < 0] = "0"
    #Confusion matrix
    confMat = confusionMatrix(data = trainData$predictions, reference = trainData$diagnosis, positive = "1")
  
    trainAccuracy[N] = confMat$overall[1]
    trainSensitivity[N] = confMat$byClass[1]
    trainSpecificity[N] = confMat$byClass[2]
    trainPPV[N] = confMat$byClass[3]
    trainNPV[N] = confMat$byClass[4]
    
    
    trainData$predictions = as.numeric(trainData$predictions)
  
    #Calculate area under the curve
    rocANS = roc(response = trainData$diagnosis, predictor = trainData$predictions)
    
    trainAUC[N] = rocANS$auc
  
#TEST model on test data_______________________________________________________________
      

    #Predict
    testData$Perc = predict(model, testData)
    
    #calculate probabilities

    testData$predictions[testData$Perc > 0] = "1"
   
    testData$predictions[testData$Perc < 0] = "0"
    #Confusion matrix
    confMatTest = confusionMatrix(data = testData$predictions, reference = testData$diagnosis, positive = "1")
  
    testAccuracy[N] = confMatTest$overall[1]
    testSensitivity[N] = confMatTest$byClass[1]
    testSpecificity[N] = confMatTest$byClass[2]
    testPPV[N] = confMatTest$byClass[3]
    testNPV[N] = confMatTest$byClass[4]
    
    
    testData$predictions = as.numeric(testData$predictions)
  
    #Calculate area under the curve
    rocANStest = roc(response = testData$diagnosis, predictor = testData$predictions)
    
    testAUC[N] = rocANStest$auc
    
    
    
    
    N = N+1
  }

  crossValTrainResults = data.frame(trainAccuracy, trainSensitivity, trainSpecificity, trainPPV, trainNPV, trainAUC)
  crossValTestResults = data.frame(testAccuracy, testSensitivity, testSpecificity, testPPV, testNPV, testAUC)
  
  
  #Take the means for overall performance
  trainResults = unlist(lapply(crossValTrainResults, mean))
  testResults = unlist(lapply(crossValTestResults, mean))
  
  if (n == 1){
    dfResultsAll = data.frame(trainResults, testResults)
    #rename colnames
    colnames = c(str_c("train_", feature), str_c("test_", feature))
    
    colnames(dfResultsAll) = colnames
    n = n+1
  }
  else{
    dfResultsAll = data.frame(dfResultsAll, trainResults, testResults)
    
    colnames = c(colnames, str_c("train_", feature), str_c("test_", feature))
    
    colnames(dfResultsAll) = colnames
    
  }

}


row.names(dfResultsAll) = c("accuracy", "sensitivity", "specificity", "PPV", "NPV", "AUC")




``` 
Train rqa_RR auc = 0.524
Test rqa_RR auc = 0.518


### Question 3 - Which combination of acoustic predictors is best for diagnosing schizophrenia?


Now it's time to go wild! Use all (voice-related) variables and interactions you can think of. Compare models and select the best performing model you can find.

Remember:
- Cross-validation or AIC are crucial to build the best model!
- After choosing the model, train it on all the data you have
- Save the model: save(modelName, file = "BestModelForever.rda")
- Create a Markdown that can: a) extract the features from new pitch files (basically your previous markdown), b) load your model (e.g. load("BestModelForever.rda")), and c) predict the diagnosis in the new dataframe.
Send it to Celine and Riccardo by Monday (so they'll have time to run it before class)-
```{r}

#Make models
stringMultipleModels = c("diagnosis ~ 1 + scale(range) + scale(rqa_RR) + (1|study)",
"diagnosis ~ 1 + scale(range) + scale(rqa_RR) + scale(rqa_ENTR) + (1|study)",
"diagnosis ~ 1 + scale(range) * scale(rqa_RR) + scale(rqa_ENTR) + (1|study)",
"diagnosis ~ 1 + scale(range) * scale(rqa_RR) * scale(rqa_ENTR) + (1|study)",
"diagnosis ~ 1 + scale(range) * scale(rqa_RR) + (1|study)",
"diagnosis ~ 1 + scale(rqa_RR) + scale(rqa_ENTR) + (1|study)",
"diagnosis ~ 1 + scale(rqa_RR) * scale(rqa_ENTR) + (1|study)")


modelName <- c("Q3_model1", "Q3_model2", "Q3_model3", "Q3_model4", "Q3_model5", "Q3_model6", "Q3_model7")


#Crossvalidation _______________________________________________________________


#number of folds
k =20
#create folds
folds = createFolds(unique(schiData$SUBJ), k = k, list = T, returnTrain = F)
library(stringr)
n = 1

#First loop, identifying models
for (indModel in stringMultipleModels){
  print(indModel)
  #Create empty variables
  trainAccuracy = NULL
  trainSensitivity = NULL
  trainSpecificity = NULL
  trainPPV = NULL
  trainNPV = NULL
  trainAUC = NULL

  testAccuracy = NULL
  testSensitivity = NULL
  testSpecificity = NULL
  testPPV = NULL
  testNPV = NULL
  testAUC = NULL
  
  #Add N for counting
  N = 1
  
  
  #crossvalidate loop
  
  for (fold in folds){
    testData = subset(schiData, SUBJ %in% fold)
    trainData = subset(schiData, !(SUBJ %in% fold))
    
#Train model on train data_______________________________________________________________

    model = glmer(indModel, trainData, family = binomial)

    #Predict
    trainData$Perc = predict(model, trainData)
    
    #Again we don't do an invertlogit, because Riccardo told me it wasn't necessary! I still trust you!So set them as 0
  
    trainData$predictions[trainData$Perc > 0] = "1"
 
    trainData$predictions[trainData$Perc < 0] = "0"
    #Confusion matrix
    confMat = confusionMatrix(data = trainData$predictions, reference = trainData$diagnosis, positive = "1")
  
    trainAccuracy[N] = confMat$overall[1]
    trainSensitivity[N] = confMat$byClass[1]
    trainSpecificity[N] = confMat$byClass[2]
    trainPPV[N] = confMat$byClass[3]
    trainNPV[N] = confMat$byClass[4]
    
    
    trainData$predictions = as.numeric(trainData$predictions)
  
    #Calculate area under the curve
    rocANS = roc(response = trainData$diagnosis, predictor = trainData$predictions)
    
    trainAUC[N] = rocANS$auc

#Test model on test data_______________________________________________________________
    
    #Predict
    testData$Perc = predict(model, testData)
    
    #calculate probabilities
    #If the percentage is above 0.5 we predict schizophrenia
    testData$predictions[testData$Perc > 0.5] = "1"
    #If the percentage is under 0.5 we predict control
    testData$predictions[testData$Perc < 0.5] = "0"
    #Confusion matrix
    confMatTest = confusionMatrix(data = testData$predictions, reference = testData$diagnosis, positive = "1")
  
    testAccuracy[N] = confMatTest$overall[1]
    testSensitivity[N] = confMatTest$byClass[1]
    testSpecificity[N] = confMatTest$byClass[2]
    testPPV[N] = confMatTest$byClass[3]
    testNPV[N] = confMatTest$byClass[4]
    
    
    testData$predictions = as.numeric(testData$predictions)
  
    #Calculate area under the curve
    rocANStest = roc(response = testData$diagnosis, predictor = testData$predictions)
    
    testAUC[N] = rocANStest$auc
    
    
    
    
    N = N+1
  }

  crossValTrainResults = data.frame(trainAccuracy, trainSensitivity, trainSpecificity, trainPPV, trainNPV, trainAUC)
  crossValTestResults = data.frame(testAccuracy, testSensitivity, testSpecificity, testPPV, testNPV, testAUC)
  
  
  #Take the means for overall performance
  trainResults = unlist(lapply(crossValTrainResults, mean))
  testResults = unlist(lapply(crossValTestResults, mean))
  
  if (n == 1){
    dfResultsMultiple = data.frame(trainResults, testResults)
    #rename colnames
    colnames = c(str_c("train_", modelName[n]), str_c("test_", modelName[n]))
    
    colnames(dfResultsMultiple) = colnames
    n = n+1
  }
  else{
    dfResultsMultiple = data.frame(dfResultsMultiple, trainResults, testResults)
    
    colnames = c(colnames, str_c("train_", modelName[n]), str_c("test_", modelName[n]))
    
    colnames(dfResultsMultiple) = colnames
    n = n+1
  }
print(modelName[n])
}


row.names(dfResultsMultiple) = c("accuracy", "sensitivity", "specificity", "PPV", "NPV", "AUC")



```


### Question 4: Properly report the results

METHODS SECTION: how did you analyse the data?

RESULTS SECTION: can you diagnose schizophrenia based on voice? which features are used? Comment on the difference between the different performance measures.
```{r}

#I report this question throughout my report. 
```

### Bonus question 5

You have some additional bonus data involving speech rate, pauses, etc. Include them in your analysis. Do they improve classification?

### Bonus question 6

Logistic regression is only one of many classification algorithms. Try using others and compare performance. Some examples: Discriminant Function, Random Forest, Support Vector Machine, etc. The package caret provides them.
