---
title: "Assignment2_Part1_VoiceInSchizophrenia"
author: "Riccardo Fusaroli"
date: "July 17, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Assignment 2 - Part 1 - Assessing voice in schizophrenia

Schizophrenia has been associated with "inappropriate" voice, sometimes monotone, sometimes croaky. A few studies indicate that pitch might be an index of schizophrenia. However, an ongoing meta-analysis of the literature (which you will have a go at in the last assignment) indicates that pitch mean and standard deviation are only weak indicators of diagnosis. Can we do better with our new fancy complex skills?

The corpus you are asked to analyse is a set of voice recordings from people with schizophrenia (just after first diagnosis) and 1-1 matched controls (on gender, age, education). Each participant watched 10 videos of triangles moving across the screen and had to describe them (so you have circa 10 recordings per person). I have already extracted the pitch once every 10 milliseconds and you will have to use this data to assess differences in the voice.

N.B. Question to be answered via email to Celine: can you characterize voice in schizophrenia as acoustically different? Report the methods you used to answer this question and the results from the analyses. Add a couple of lines trying to interpret the results (make sense of the difference). E.g. People with schizophrenia tend to have high-pitched voice, and present bigger swings in their prosody than controls. Add a couple of lines describing limitations of the data/analyses if any is relevant.

N.B. There are looots of files to be dealt with. Probably too many for your computer. This is a challenge for you. Some (complementary) possible strategies: You can select a subset of files only (and you have to justify your choice). You can learn how to use the apply() or map() functions. You can coordinate with classmates.


```{r}
#Set directory
setwd("~/R/R - Datascripts/Assignment 3")
#Load library
library(ggplot2);library(pastecs);library(rmarkdown);library(tidyr);
library(dplyr);library(QuantPsyc);library(VIF);library(stringr);library(tidytext);library(DHARMa)
library(lme4);library(lmerTest);library(psych);library(MuMIn);library(tidyverse);library(magrittr);library(Metrics);library(caret);library(simr);library(readtext);library(crqa);library(readr)
#Load dataset from part 1
Artic = read.delim('Articulation.txt', sep = ',')
DemoData = read.delim('DemoData.txt', sep = '.')
Pitch1 = read.table("~/R/R - Datascripts/Assignment 3/Pitch/Study1D0S101T1_f0.txt", header=T)


#Load data
files = list.files(path = "~/R/R - Datascripts/Assignment 3/Pitch",pattern="*.txt")
print(files)
#Artic data. 0 means control, 1 = Schizo. Number after 0 and 1 identifies two similiarly matched participants. They have similiar traits, so schizo is matched with a control.
#T 1, 2, 3 = video 1 they have to describe, video 2 etc
#

```

1. In the course of this assignment you have to first select one datafile and figure out how to:
- Extract "standard" descriptors of pitch: Mean, standard deviation, range
- Extract less "standard" descriptors of pitch you can think of (e.g. median, iqr, mean absoluted deviation, coefficient of variation)
- Extract "complex" descriptors: recurrence quantification analysis

```{r}
#Extract "standard" descriptors
range(Pitch1$f0)
mean(Pitch1$f0)
sd(Pitch1$f0)


range(Pitch1$time)
mean(Pitch1$time)
sd(Pitch1$time)
#Frequency M = 139,92, SD = 31,38, 142,01.

#Extract "less standard"
stat.desc(Pitch1)
#Frequency median = 125,59, SE = 2,47, Coef.var = 0,224.

#Extract "complex descriptors".

#Delay parameter threshold(T); can be estimated using the mutual average information function.
#T is the basis of unfolding the one-dimensional time-series into a multidimensional phase-space.

library(tseriesChaos)
# run Average Mutual Information
mutual(Pitch1$f0, lag.max = 50)
#I chose the lowest value in my AMI plot to make the most conservative analysis. T = 22.

# Embedding parameter D; can be estimated using the false-nearest neighbor function.
# If D = 1, we do not need to embed.
# If D > 1, we want to unfold the one-dimensional time-series into as many dimensions so that we exhaust all information about higher-dimensional dynamics contained in the time-series, but not more.

# m: embedding dimension D
# d: delay T
# t: an additional parameter, the Theiler-window, that can help to increase the reliability of the false-nearest neighbor analysis and recurrence statistics by removing unwanted short-term temporal correlations.


# run false-nearestneighbor analysis
fnn = false.nearest(Pitch1$f0, m=5, d=22, t=0)
plot(fnn)

#Install package
#install.packages("crqa")
# ??crqa
mlpar = list(lgM =  35, radiusspan = 400, radiussample = 10, normalize = 0, rescale = 1, mindiagline = 2, 
             minvertline = 2, tw = 0, whiteline = FALSE, recpt = FALSE, fnnpercent = 10, typeami = "maxlag")
optpar <- optimizeParam(Pitch1,Pitch1,mlpar); optpar

#I find the parameters for my crqa model:
#

Results <- try(crqa(Pitch1, Pitch1, embed = 3, delay = 30, radius = 2, normalize = 0, rescale = 2, mindiagline = 2, minvertline = 2))
Results

RP=Results$RP
RP = matrix(as.numeric(RP), nrow = ncol(RP)) 
cols = c("white","blue4")
image(RP, xlab = "", ylab = "", col = cols)
#Falsen
#RR 


#Delay
#Package, function Mutual(), tells muttual information. Tells how easy it is to predict the next datapoints. How strongly correlated two points in time. Trying to predict two points in the future, less correlation. So it should increase over time. We should try to find the optimal delay. Take the minium possible delay, really conservative and removes some data.

#Second way we look at when the slopes changes between data lines, so down, down, down, up. We stopÃ¥ here.Less conservative, keeps more data. 

#Use optimizeparam function. Will always pick the most conservative. Will give error embed*delay longer than tsm. We plot in timeseries in x-axis, but because we add a delay in y axis. We have to add x-time ontop of the x-axis time. We end up removing lots of data. If we use try() around it, it will run it and remove these.

#Use CRQA()

```

2. Second you will have to turn the code into a function and loop through all the files (or even better use apply/sapply/lapply)
- Remember to extract the relevant information from the file names (Participant, Diagnosis, Trial, Study)

```{r}
#List all files


#Make loop
#The simplest loop

#Calculate mean, SD, and run crqa
Mp = NULL
SDp = NULL
radius = NULL
embed = NULL
delay = NULL

N = 1

files <- list.files(path = "~/R/R - Datascripts/Assignment 3/Pitch", pattern="*.txt")

for (i in files) {
  #Can use read_csv with ReadR package. Will read txt files aswell.
  x = read.table(i, header = T)
  #When did we record the pitch. We care about the F0 (pitch), we don't care about time. Drop time from x.
  #x = x$F0
}  
  Mp[N]= mean(x, na.rm = T)
  SDp[N] = SD(x,na.rm = T)
  #It's best to start from one
    N = N +1
  #We want to save each loop in a dataframe: Mean and SD as columns.
  #We first create these variables. 
  #We create empty variables
  
  
  #LOOK AT REGULAR EXPRESSIONS - Regex.
  #Extract ID. Use a stringer extract strings, Use package StringR
  #Create new empty variable 
  particpant = Null
  ID = str_extract(i, "S+\\d+")#in i find big S (string), like find S102 (study S102) \\ regular expression, search for digits, + means look for mulitple digtigts. Tell it look in files. Problem using S102, subjects ID's is not more meaningful as digits.
  particpant[n] = str_extract(ID, "\\d+") #Use ID, it will look for ID numbers digits.
  #use above code (kinda) to extract Diagnosis.
  
  mlpar = list(lgM =  35, radiusspan = 400, radiussample = 10, normalize = 0, rescale = 1,     mindiagline = 2, 
             minvertline = 2, tw = 0, whiteline = FALSE, recpt = FALSE, fnnpercent = 10, typeami = "maxlag")
  
#RQUA, find Delay, Embed and Radius. Use optimize function optimizeparam()
#One school of thought: Unfold the timephase in the best phasespace.
#Other school of thought: You need to choose the same Delay, Embed and Radius for all timeseries. You want to maximise comparability of timeseries.
#Both work.
  
#Always argue for your choice of extracting paramters!
  
#output of optimize. 
#Issues with optimizeParam(). There's 3 dimensions Ex. Embed is longer than delay in timeseries. Embed could be 3, delay 50 and timeseries 1 sec.
#We want to put timeseries on both axis. If it's a 100 secons, we have to throw everything above 100 on y and x-axis away. On the third dimension, we have to delay. Problem we want to set it to 101, but there is none, so it throws everything away.
#There is a paramter in optimize lgM, means lag maximum. Use 15. They found the lag by testing numbers in files above 100, that kept most data. Ex. if you record a voice with a microphone, the vibrations from first test could continue slightly into the second, polluting data. So you find a cutoff.

#When we look at recurrence plot thats' either blank or black, we can't read anything from it. 
#Recurrence rate is discontinous. Sometimes you can't optimize paramters, you can try and relax the min and max radius. Less precise comparability between min and max if there's a larger difference.
#Use min.Rec = 3 and max.rec = 5
#If we look at the plot and get a black square around 0.4, 
  #A = optimizeParam(x,x, mlpar)
#If you get embed*delay error shows up, use try(). This makes the loop continue and doesn't crash. You want the loop to keep running if it gets an error. 
  A = try(optimizeParam(x,x,mlpar))
  if(length(A)<2{
    delay[N] = NA #Delay of datapoints
    embed[N] = NA
    radius[N] = NA
  })
    else(delay[N] = A$Delay)
}
#After loop write into dataframe
Df = data.frame(participant, Mp, SDp)

    #Save delay and embed and radius into a dataframe. Now you have optimal paramters.
    #You now find a delay, embed and radius for all files.
    #So find median for all delays
    Delay = Mean(Df$Delay, na.rm = TRUE)
    #You can build a second loop just find CRQA information and save into dataframe for each files


#After loop write into dataframe.

#Second loop would look like this
for (i in files) {
  #Can use read_csv with ReadR package. Will read txt files aswell.
  x = read.table(i)
  x = x$F0
  # When did we record the pitch. We care about the F0 (pitch), we don't care about time. Drop time from x.
  # Use paramters found in the first loop
  CRQA(x,x...)
  RR[N] = A$Rec
  DET[N] = A$DET
  
  # Write into a csv file
  write.csv(dataframe, file = "filenamewhatever.csv")

}

```



Josephine loop
```{r}
#Extract list of files
files = list.files(path = "~/R/R - Datascripts/Assignment 3/Pitch",pattern="*.txt")
#setWD
setwd("~/R/R - Datascripts/Assignment 3/Pitch")

participant = NULL
diagnosis = NULL
trial = NULL
study = NULL
delay = NULL
radius = NULL
embed = NULL




n = 1




#Extract optimal parameters
for (file in files){
  #print(file)
  
  
  #read the specific file
  tempFile = read.table(file, header = T)
  
  #get participant number information
  #would like to extract seperate participant, trial and diagnosis info
  study[n] = substring(file, 6, 6)
  diagnosis[n] = substring(file, 8, 8)
  participant[n] = substring(sub("T.*", "", file),8)
  trial[n] = sub(".T*", "", sub("\\_.*", "", sub("^[^T]*", "", file)))
  
  
  #optimizeparam functions
  par = list(lgM = 50, steps = seq(1, 6, 1), radiusspan = 100, radiussample = 40, normalize = 0, rescale = 0, mindiagline = 2, minvertline = 2, tw = 0, whiteline = F, recpt = FALSE, fnnpercent = 10, typeami = "mindip")
  ans = try(optimizeParam(tempFile$f0, tempFile$f0, par))
  
  #print(ans)
  
  #if statement: If having problem with optimizeparam, it will append NA
  if (length(ans) > 1){
    radius[n] = ans[1]
    embed[n] = ans[2]
    delay[n] = ans[3]
    }
  
  else {
    radius[n] = NA
    embed[n] = NA
    delay[n] = NA
    
  }
  
  n = n+1
  print(n)
  
}




#Need to unlist it in order to make it a list!
delay = unlist(delay)
embed = unlist(embed)
radius = unlist(radius)


df = data.frame(participant, diagnosis, study, trial, embed, delay, radius)



#Write to datafile to avoid looping through all of them again
write.csv(df, file = "dataExtractSchizo.csv", row.names = F)


```


Make the loop for your CRQA


```{r}
#setWD for chunk
setwd("~/R/R - Datascripts/Assignment 3/Pitch")
#calculate median delay, radius and embed
radiusOptimal = median(radius, na.rm = T)
embedOptimal = median(embed, na.rm = T)
delayOptimal = median(delay, na.rm = T)


#crqaextract
N = 1
rqa_RR = NULL
rqa_DET = NULL
rqa_NRLINE = NULL
rqa_maxL = NULL
rqa_L = NULL
rqa_ENTR = NULL
rqa_rENTR = NULL
rqa_LAM = NULL
rqa_TT = NULL
rqa_RP = NULL


for (file in files){
  print(file)
  
  #read the specific file
  tempFile = read.table(file, header = T)
  
  #Make CRQA
  results = try(crqa(tempFile$f0, tempFile$f0, delay = delayOptimal, embed = embedOptimal, radius = radiusOptimal, normalize = 0, rescale = 0, mindiagline = 2, minvertline = 2))
  # 
  # mlpar = list(lgM =  35, radiusspan = 400, radiussample = 10, normalize = 0, rescale = 1,     mindiagline = 2, 
  #            minvertline = 2, tw = 0, whiteline = FALSE, recpt = FALSE, fnnpercent = 10, typeami = "maxlag")
  
  if (length(results) > 1){
    #Write to lists
    rqa_RR[N] = results[1]
    rqa_DET[N] = results[2]
    rqa_NRLINE[N] = results[3]
    rqa_maxL[N] = results[4]
    rqa_L[N] = results[5]
    rqa_ENTR[N] = results[6]
    rqa_rENTR[N] = results[7]
    rqa_LAM[N] = results[8]
    rqa_TT[N] = results[9]
    rqa_RP[N] = results[10]
  }
  else{
    rqa_RR[N] = NA
    rqa_DET[N] = NA
    rqa_NRLINE[N] = NA
    rqa_maxL[N] = NA
    rqa_L[N] = NA
    rqa_ENTR[N] = NA
    rqa_rENTR[N] = NA
    rqa_LAM[N] = NA
    rqa_TT[N] = NA
    rqa_RP[N] = NA
  }
  
  
  N = N+1
  
  }

#Unlist variables
rqa_RR = unlist(rqa_RR)
rqa_DET = unlist(rqa_DET)
rqa_NRLINE = unlist(rqa_NRLINE)
rqa_maxL = unlist(rqa_maxL)
rqa_L = unlist(rqa_L)
rqa_ENTR = unlist(rqa_ENTR)
rqa_LAM = unlist(rqa_LAM)
rqa_TT = unlist(rqa_TT)


#Add to our lovely dataframe
df_new = data.frame(df, rqa_RR, rqa_DET, rqa_NRLINE, rqa_maxL, rqa_L, rqa_ENTR, rqa_LAM, rqa_TT)



```


```{r}
#setwd
("~/R/R - Datascripts/Assignment 3/Pitch")
mean = NULL
median = NULL
stdDev = NULL
range = NULL
IQR = NULL
coefOfVar = NULL
meanAbsDev = NULL

N = 1

for (file in files){
  print(file)
  
  #read the specific file
  tempFile = read.table(file, header = T)
  
  #Mean
  mean[N] = mean(tempFile$f0)
  
  #median
  median [N] = median(tempFile$f0)
  
  #stdDev
  stdDev[N] = sd(tempFile$f0)
  
  #range
  range[N] = max(range(tempFile$f0))-min(range(tempFile$f0))
  
  #Interquartile range
  IQR[N] = IQR(tempFile$f0)
  
  #CV
  coefOfVar[N] = stdDev[N]/mean[N]*100
  
  meanAbsDev[N] = mad(tempFile$f0)
  
  N = N+1
  
}


df = data.frame(df_new, mean, median, stdDev, range, IQR, coefOfVar, meanAbsDev)

#write to CSV
write.csv(df, file = "dataExtractSchizo.csv", row.names = F)

```
3. Make one model per acoustic feature and test whether you can observe significant difference due to Diagnosis. Tip: Which other fixed factors should you control for (that is, include in the model)? Which random ones?
- Bonus points: cross-validate the model and report the betas and standard errors from all rounds to get an idea of how robust the estimates are. 
3a. Is study a significant predictor in these models? What should you infer from this? Does study interact with diagnosis? What should you infer from this?

```{r}
#Load in newly made datafile for ease
setwd("~/R/R - Datascripts/Assignment 3")
df3 = read.delim('dataExtractSchizo.csv', sep = ',')


#Use scale() for fixed effects
df3$trial_SC<-scale(df3$trial)
df3$study_SC <- scale(df3$study)
str(df3)

#RR
pitchRR <- lmer(rqa_RR ~ diagnosis + trial_SC + study_SC + (1|study) + (1+diagnosis|participant), df3, REML = FALSE)
summary(pitchRR)
anova(pitchRR)

#DET
pitchDET <- lmer(rqa_DET ~ diagnosis + trial_SC + study_SC + (1|study) + (1+diagnosis|participant), df3, REML = FALSE)
summary(pitchDET)

#ENTR
pitchENTR <- lmer(rqa_ENTR ~ diagnosis + trial_SC + study_SC + (1|study) + (1+diagnosis|participant), df3, REML = FALSE)

#L
pitchL <- lmer(rqa_L ~ diagnosis + trial_SC + study_SC + (1|study) + (1+diagnosis|participant), df3, REML = FALSE)

#LAM
pitchLAM <- lmer(rqa_LAM ~ diagnosis + trial_SC + study_SC + (1|study) + (1+diagnosis|participant), df3, REML = FALSE)

#maxL
pitchmaxL <- lmer(rqa_maxL ~ diagnosis + trial_SC + study_SC + (1|study) + (1+diagnosis|participant), df3, REML = FALSE)

#NRLINE
pitchNRLINE <- lmer(rqa_NRLINE ~ diagnosis + trial_SC + study_SC + (1|study) + (1+diagnosis|participant), df3, REML = FALSE)

#ENTR
pitchENTR <- lmer(rqa_ENTR ~ diagnosis + trial_SC + study_SC + (1|study) + (1+diagnosis|participant), df3, REML = FALSE)

#RP
pitchRP <- lmer(rqa_RP ~ diagnosis + trial_SC + study_SC + (1|study) + (1+diagnosis|participant), df3, REML = FALSE)

#TT
pitchTT <- lmer(rqa_TT ~ diagnosis + trial_SC + study_SC + (1|study) + (1+diagnosis|participant), df3, REML = FALSE)


#Can we take all the information into our model, find random structures to put into our model

PRR(pitch)(and RR from loop) ~ D + Trial + #Each trial can also be systematic, we think it has an influence on our dependant variable, it has to be viewed as a fixed effect aswell as a random.
                    
                    (1|Study1) + #The range of pitch can be different for different studies. In one they removed outliers, in one they didn't. You control for difference in variablity for different studies.
                    (1+Trial|ID) + #Each person has different slopes for pitch over trials. Ex. person gets tired and pitch lowers over trial.
  
#There are two particpants for each ID, so two 102, two 103. One is a control, so one Skitzo, one typ. In a study in copenhagen, you might have people with different dialect, different sexs. So particpants need controls very similar to them. 
#Modelling this. The two particpants have the same data (more or less) we have to tell the model that these two are different people. The match of these could make your study more powerfull, so it's not ideal to rename them. 
                    (1+D|ID) + #By putting the diagnosis over the matching ID, we take into account that the match of participants might vary. We allow random slopes and intercepts. If we don't, we assume the match don't vary by pairs, with the random effect we say that they could vary.  We seperate control and schitzo.

#Matching two, we can expect that the control in order to match the schitzo might have other issues causing them to match - like very low education. Not really normal in DK. 
                    (1+D+Trial|ID) #We can take the random effects for ID, trial and diagnosis and put it into one (). Trial controls for every participant over trials.

#If these two don't converge, you would assume that Diagnosis and Trial aren't interacting and you'd say:
                    (1+D|ID) + (1+Trial|ID)
#The random effect of diagnosis, would mean that severity of schitzo would differ from study aswell. 

#Interpret results
#Ex.
Beta = 5 # schitzo has a higher recurrance rate
SE = 1.2 # 
p-value = p < 0.005 #significantly higher

#L (average length of lines in plot (recurrance line))= Look at group of snippets. Look at snippets over time, seeing changes in pitch. We look at line of plot, like 5. What does that mean. We know that each datapoint is 10 seconds. We know there's a delay of 3 and embed of 2. We want to identify the unit in our phasespace. We have 2 datapoints each 10 seconds, seperated by 3. We want to see how long these extend. We can see the patteren that recurs is between 1-8, 90ms. so x2-x5 (sep by 3), x3-x6, x5-x8.

#Main point of this is to "build" a machine that can determine if someone is schitzo.


```


4. Bonus Question: Compare effect size of diagnosis across the different measures. Which measure seems most sensitive?
- Tip: to compare across measures you need to put all of them on the same scale, that is, you need to "standardize" them (z-score)

5. Bonus question. In the Clinical Info file you have additional information about the participants. Which additional parameters (e.g. age, gender) should we control for? Report the effects.

6. Write a paragraph reporting methods and results

[Next assignment: can we use these measures to build a tool that diagnoses people from voice only?]

## N.B. Remember to save the acoustic features of voice in a separate file, so to be able to load them next time